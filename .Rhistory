data1$subject <- as.character(data1$subject)
data1$text <- as.character(data1$text)
# Ensure the `date` column is numeric
data1$date <- suppressWarnings(as.numeric(data1$date))
if (any(is.na(data1$date))) {
data1$date[is.na(data1$date)] <- 0  # Replace invalid numeric conversions with 0
}
# Rename target column (if exists) or create it
if ("label" %in% colnames(data1)) {
colnames(data1)[colnames(data1) == "label"] <- "target"
} else {
# Assuming `target` is derived from another column, you need to define it
data1$target <- sample(0:1, nrow(data1), replace = TRUE)  # Replace this with actual logic
}
data1$target <- as.factor(data1$target)
# Data Visualizations
numeric_cols <- data1[, sapply(data1, is.numeric), drop = FALSE]
# Histogram for numeric features
if (ncol(numeric_cols) > 0) {
ggplot(stack(numeric_cols), aes(x = values)) +
geom_histogram(fill = "blue", bins = 30, color = "black", alpha = 0.7) +
facet_wrap(~ind, scales = "free") +
ggtitle("Distribution of Numeric Features") +
xlab("Values") +
ylab("Frequency") +
theme_minimal()
} else {
cat("No numeric columns found for histogram.\n")
}
# Bar chart for class distribution
ggplot(data1, aes(x = target)) +
geom_bar(fill = "orange") +
ggtitle("Class Distribution: Fake vs Real News") +
xlab("Target (0 = Fake, 1 = Real)") +
ylab("Count") +
theme_minimal()
# Correlation Analysis
if (ncol(numeric_cols) > 1) {
correlation1 <- cor(numeric_cols, use = "complete.obs")
corrplot(correlation1, method = "circle")
} else {
cat("Insufficient numeric columns for correlation analysis.\n")
}
# Feature Selection
selected_features1 <- c(names(numeric_cols), "target")
data1 <- data1[, selected_features1, drop = FALSE]
# Split Data
set.seed(123)
train_index1 <- createDataPartition(data1$target, p = 0.8, list = FALSE)
train_data1 <- data1[train_index1, ]
test_data1 <- data1[-train_index1, ]
# Ensure target is numeric for linear regression
data1$target <- as.numeric(data1$target)
# Tuning grid for Naive Bayes
nb_tune_grid <- expand.grid(fL = 0, usekernel = TRUE, adjust = 1)
# Train Naive Bayes model with hyperparameter tuning
nb_tune_model <- train(
target ~ .,
data = train_data1,
method = "nb",
tuneGrid = nb_tune_grid,
trControl = trainControl(method = "cv", number = 5)
)
# Predictions from Naive Bayes model
nb_pred1 <- predict(nb_tune_model, newdata = test_data1)
# Ensure target is numeric for regression
data1$target <- as.numeric(data1$target)
# Remove any rows with missing values
train_data1 <- na.omit(train_data1)
# Narrow tuning grid for glmnet (Elastic Net regularization)
lr_tune_grid <- expand.grid(
alpha = c(0, 0.5, 1),  # Ridge, Elastic Net, and Lasso
lambda = c(0, 0.01)  # Narrow lambda range
)
# Train the Linear Regression model using caret
train_control <- trainControl(
method = "cv",
number = 5,  # Cross-validation with 5 folds
summaryFunction = defaultSummary  # Use RMSE, MAE, or R-squared for regression
)
# Train the model
lr_tune_model <- train(
target ~ .,
data = train_data1,
method = "glmnet",
tuneGrid = lr_tune_grid,
trControl = train_control
)
# Check if training was successful before printing the best tune parameters
if (exists("lr_tune_model")) {
print(lr_tune_model$bestTune)
}
# Check for any warnings after the training
warnings()  # Use this to identify the reason for failure
# Predictions and evaluation if the model is trained successfully
if (exists("lr_tune_model")) {
# Predictions
lr_pred1 <- predict(lr_tune_model, newdata = test_data1)
# RMSE and R-squared
rmse_value <- sqrt(mean((lr_pred1 - test_data1$target)^2))  # RMSE calculation
rsq_value <- cor(lr_pred1, test_data1$target)^2  # R-squared calculation
cat(sprintf("RMSE: %.4f\n", rmse_value))
cat(sprintf("R-squared: %.4f\n", rsq_value))
}
# Results Summary: Fake vs Real News
fake_news_count <- sum(test_data1$target == 0)
real_news_count <- sum(test_data1$target == 1)
cat(sprintf("Number of Fake News: %d\n", fake_news_count))
cat(sprintf("Number of Real News: %d\n", real_news_count))
# Load libraries
library(dplyr)
library(ggplot2)
library(caret)
library(e1071)  # For Naive Bayes
library(e1071)  # For Naive Bayes
library(corrplot)
library(Metrics)  # For Linear Regression metrics
# Load dataset
data1 <- read.csv("True.csv", stringsAsFactors = FALSE)
# Data Cleaning
data1 <- distinct(data1)  # Remove duplicates
data1[is.na(data1)] <- 0  # Replace NA with 0
# Ensure column types
data1$title <- as.character(data1$title)
data1$subject <- as.character(data1$subject)
data1$text <- as.character(data1$text)
# Ensure the `date` column is numeric
data1$date <- suppressWarnings(as.numeric(data1$date))
if (any(is.na(data1$date))) {
data1$date[is.na(data1$date)] <- 0  # Replace invalid numeric conversions with 0
}
# Rename target column (if exists) or create it
if ("label" %in% colnames(data1)) {
colnames(data1)[colnames(data1) == "label"] <- "target"
} else {
# Assuming `target` is derived from another column, you need to define it
data1$target <- sample(0:1, nrow(data1), replace = TRUE)  # Replace this with actual logic
}
data1$target <- as.factor(data1$target)
# Data Visualizations
numeric_cols <- data1[, sapply(data1, is.numeric), drop = FALSE]
# Histogram for numeric features
if (ncol(numeric_cols) > 0) {
ggplot(stack(numeric_cols), aes(x = values)) +
geom_histogram(fill = "blue", bins = 30, color = "black", alpha = 0.7) +
facet_wrap(~ind, scales = "free") +
ggtitle("Distribution of Numeric Features") +
xlab("Values") +
ylab("Frequency") +
theme_minimal()
} else {
cat("No numeric columns found for histogram.\n")
}
# Bar chart for class distribution
ggplot(data1, aes(x = target)) +
geom_bar(fill = "orange") +
ggtitle("Class Distribution: Fake vs Real News") +
xlab("Target (0 = Fake, 1 = Real)") +
ylab("Count") +
theme_minimal()
# Correlation Analysis
if (ncol(numeric_cols) > 1) {
correlation1 <- cor(numeric_cols, use = "complete.obs")
corrplot(correlation1, method = "circle")
} else {
cat("Insufficient numeric columns for correlation analysis.\n")
}
# Feature Selection
selected_features1 <- c(names(numeric_cols), "target")
data1 <- data1[, selected_features1, drop = FALSE]
# Split Data
set.seed(123)
train_index1 <- createDataPartition(data1$target, p = 0.8, list = FALSE)
train_data1 <- data1[train_index1, ]
test_data1 <- data1[-train_index1, ]
# Ensure target is numeric for linear regression
data1$target <- as.numeric(data1$target)
# Tuning grid for Naive Bayes
nb_tune_grid <- expand.grid(fL = 0, usekernel = TRUE, adjust = 1)
# Train Naive Bayes model with hyperparameter tuning
nb_tune_model <- train(
target ~ .,
data = train_data1,
method = "nb",
tuneGrid = nb_tune_grid,
trControl = trainControl(method = "cv", number = 5)
)
# Predictions from Naive Bayes model
nb_pred1 <- predict(nb_tune_model, newdata = test_data1)
# Ensure target is numeric for regression
data1$target <- as.numeric(data1$target)
# Remove any rows with missing values
train_data1 <- na.omit(train_data1)
# Narrow tuning grid for glmnet (Elastic Net regularization)
lr_tune_grid <- expand.grid(
alpha = c(0, 0.5, 1),  # Ridge, Elastic Net, and Lasso
lambda = c(0, 0.01)  # Narrow lambda range
)
# Train the Linear Regression model using caret
train_control <- trainControl(
method = "cv",
number = 5,  # Cross-validation with 5 folds
summaryFunction = defaultSummary  # Use RMSE, MAE, or R-squared for regression
)
# Train the model
lr_tune_model <- train(
target ~ .,
data = train_data1,
method = "glmnet",
tuneGrid = lr_tune_grid,
trControl = train_control
)
# Load libraries
library(dplyr)
library(ggplot2)
library(caret)
library(e1071)  # For Naive Bayes
library(corrplot)
library(Metrics)  # For Linear Regression metrics
# Load datasets for Fake and Real news
data1 <- read.csv("True.csv", stringsAsFactors = FALSE)  # Real news
data2 <- read.csv("Fake.csv", stringsAsFactors = FALSE)  # Fake news
# Add a 'target' column to each dataset to indicate fake (0) or real (1) news
data1$target <- 1  # Real news is labeled as 1
data2$target <- 0  # Fake news is labeled as 0
# Combine both datasets into one
data <- bind_rows(data1, data2)
# Data Cleaning
data <- distinct(data)  # Remove duplicates
data[is.na(data)] <- 0  # Replace NA with 0
# Ensure column types
data$title <- as.character(data$title)
data$subject <- as.character(data$subject)
data$text <- as.character(data$text)
# Ensure the `date` column is numeric
data$date <- suppressWarnings(as.numeric(data$date))
if (any(is.na(data$date))) {
data$date[is.na(data$date)] <- 0  # Replace invalid numeric conversions with 0
}
# Data Visualizations
numeric_cols <- data[, sapply(data, is.numeric), drop = FALSE]
# Histogram for numeric features
if (ncol(numeric_cols) > 0) {
ggplot(stack(numeric_cols), aes(x = values)) +
geom_histogram(fill = "blue", bins = 30, color = "black", alpha = 0.7) +
facet_wrap(~ind, scales = "free") +
ggtitle("Distribution of Numeric Features") +
xlab("Values") +
ylab("Frequency") +
theme_minimal()
} else {
cat("No numeric columns found for histogram.\n")
}
# Bar chart for class distribution
ggplot(data, aes(x = target)) +
geom_bar(fill = "orange") +
ggtitle("Class Distribution: Fake vs Real News") +
xlab("Target (0 = Fake, 1 = Real)") +
ylab("Count") +
theme_minimal()
# Correlation Analysis
if (ncol(numeric_cols) > 1) {
correlation1 <- cor(numeric_cols, use = "complete.obs")
corrplot(correlation1, method = "circle")
} else {
cat("Insufficient numeric columns for correlation analysis.\n")
}
# Feature Selection
selected_features <- c(names(numeric_cols), "target")
data <- data[, selected_features, drop = FALSE]
# Split Data
set.seed(123)
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
# Split Data
set.seed(150)
train_index <- createDataPartition(data$target, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# Ensure target is numeric for linear regression
train_data$target <- as.numeric(train_data$target)
# Tuning grid for Naive Bayes
nb_tune_grid <- expand.grid(fL = 0, usekernel = TRUE, adjust = 1)
# Train Naive Bayes model with hyperparameter tuning
nb_tune_model <- train(
target ~ .,
data = train_data,
method = "nb",
tuneGrid = nb_tune_grid,
trControl = trainControl(method = "cv", number = 5)
)
# Predictions from Naive Bayes model
nb_pred <- predict(nb_tune_model, newdata = test_data)
# Ensure target is numeric for regression
train_data$target <- as.numeric(train_data$target)
# Remove any rows with missing values
train_data <- na.omit(train_data)
# Narrow tuning grid for glmnet (Elastic Net regularization)
lr_tune_grid <- expand.grid(
alpha = c(0, 0.5, 1),  # Ridge, Elastic Net, and Lasso
lambda = c(0, 0.01)  # Narrow lambda range
)
# Train the Linear Regression model using caret
train_control <- trainControl(
method = "cv",
number = 5,  # Cross-validation with 5 folds
summaryFunction = defaultSummary  # Use RMSE, MAE, or R-squared for regression
)
# Train the model
lr_tune_model <- train(
target ~ .,
data = train_data,
method = "glmnet",
tuneGrid = lr_tune_grid,
trControl = train_control
)
# Check if training was successful before printing the best tune parameters
if (exists("lr_tune_model")) {
print(lr_tune_model$bestTune)
}
# Check for any warnings after the training
warnings()  # Use this to identify the reason for failure
# Predictions and evaluation if the model is trained successfully
if (exists("lr_tune_model")) {
# Predictions
lr_pred <- predict(lr_tune_model, newdata = test_data)
# RMSE and R-squared
rmse_value <- sqrt(mean((lr_pred - test_data$target)^2))  # RMSE calculation
rsq_value <- cor(lr_pred, test_data$target)^2  # R-squared calculation
cat(sprintf("RMSE: %.4f\n", rmse_value))
cat(sprintf("R-squared: %.4f\n", rsq_value))
}
# Results Summary: Fake vs Real News
fake_news_count <- sum(test_data$target == 0)
real_news_count <- sum(test_data$target == 1)
cat(sprintf("Number of Fake News: %d\n", fake_news_count))
cat(sprintf("Number of Real News: %d\n", real_news_count))
# Load libraries
library(dplyr)
library(ggplot2)
library(caret)
library(e1071)  # For Naive Bayes
library(corrplot)
library(Metrics)  # For Linear Regression metrics
# Load dataset
data1 <- read.csv("True.csv", stringsAsFactors = FALSE)
data2 <- read.csv("Fake.csv", stringsAsFactors = FALSE)
# Data Cleaning
data1 <- distinct(data1)  # Remove duplicates
data1[is.na(data1)] <- 0  # Replace NA with 0
# Ensure column types
data1$title <- as.character(data1$title)
data1$subject <- as.character(data1$subject)
data1$text <- as.character(data1$text)
# Ensure the `date` column is numeric
data1$date <- suppressWarnings(as.numeric(data1$date))
if (any(is.na(data1$date))) {
data1$date[is.na(data1$date)] <- 0  # Replace invalid numeric conversions with 0
}
# Rename target column (if exists) or create it
if ("label" %in% colnames(data1)) {
colnames(data1)[colnames(data1) == "label"] <- "target"
} else {
# Assuming `target` is derived from another column, you need to define it
data1$target <- sample(0:1, nrow(data1), replace = TRUE)  # Replace this with actual logic
}
data1$target <- as.factor(data1$target)
# Data Visualizations
numeric_cols <- data1[, sapply(data1, is.numeric), drop = FALSE]
# Histogram for numeric features
if (ncol(numeric_cols) > 0) {
ggplot(stack(numeric_cols), aes(x = values)) +
geom_histogram(fill = "blue", bins = 30, color = "black", alpha = 0.7) +
facet_wrap(~ind, scales = "free") +
ggtitle("Distribution of Numeric Features") +
xlab("Values") +
ylab("Frequency") +
theme_minimal()
} else {
cat("No numeric columns found for histogram.\n")
}
# Bar chart for class distribution
ggplot(data1, aes(x = target)) +
geom_bar(fill = "orange") +
ggtitle("Class Distribution: Fake vs Real News") +
xlab("Target (0 = Fake, 1 = Real)") +
ylab("Count") +
theme_minimal()
# Correlation Analysis
if (ncol(numeric_cols) > 1) {
correlation1 <- cor(numeric_cols, use = "complete.obs")
corrplot(correlation1, method = "circle")
} else {
cat("Insufficient numeric columns for correlation analysis.\n")
}
# Feature Selection
selected_features1 <- c(names(numeric_cols), "target")
data1 <- data1[, selected_features1, drop = FALSE]
# Split Data
set.seed(123)
train_index1 <- createDataPartition(data1$target, p = 0.8, list = FALSE)
train_data1 <- data1[train_index1, ]
test_data1 <- data1[-train_index1, ]
# Ensure target is numeric for linear regression
data1$target <- as.numeric(data1$target)
# Tuning grid for Naive Bayes
nb_tune_grid <- expand.grid(fL = 0, usekernel = TRUE, adjust = 1)
# Train Naive Bayes model with hyperparameter tuning
nb_tune_model <- train(
target ~ .,
data = train_data1,
method = "nb",
tuneGrid = nb_tune_grid,
trControl = trainControl(method = "cv", number = 5)
)
# Predictions from Naive Bayes model
nb_pred1 <- predict(nb_tune_model, newdata = test_data1)
# Ensure target is numeric for regression
data1$target <- as.numeric(data1$target)
# Remove any rows with missing values
train_data1 <- na.omit(train_data1)
# Narrow tuning grid for glmnet (Elastic Net regularization)
lr_tune_grid <- expand.grid(
alpha = c(0, 0.5, 1),  # Ridge, Elastic Net, and Lasso
lambda = c(0, 0.01)  # Narrow lambda range
)
# Train the Linear Regression model using caret
train_control <- trainControl(
method = "cv",
number = 5,  # Cross-validation with 5 folds
summaryFunction = defaultSummary  # Use RMSE, MAE, or R-squared for regression
)
# Train the model
lr_tune_model <- train(
target ~ .,
data = train_data1,
method = "glmnet",
tuneGrid = lr_tune_grid,
trControl = train_control
)
# Load dataset
data1 <- read.csv("WELFake_Dataset.csv", stringsAsFactors = FALSE)
# Load libraries
library(dplyr)
library(ggplot2)
library(caret)
library(randomForest)
library(e1071)
library(corrplot)
# Data Cleaning
data1 <- distinct(data1)  # Remove duplicates
data1[is.na(data1)] <- 0  # Replace NA with 0
colnames(data1)[colnames(data1) == "label"] <- "target"
data1$target <- as.factor(data1$target)  # Convert target to factor
# Data Visualizations
# 1. Histogram for numeric features
numeric_cols <- data1[, sapply(data1, is.numeric), drop = FALSE]
ggplot(stack(numeric_cols), aes(x = values)) +
geom_histogram(fill = "blue", bins = 30, color = "black", alpha = 0.7) +
facet_wrap(~ind, scales = "free") +
ggtitle("Distribution of Numeric Features") +
xlab("Values") +
ylab("Frequency") +
theme_minimal()
# 2. Bar chart for class distribution
ggplot(data1, aes(x = target)) +
geom_bar(fill = "orange") +
ggtitle("Class Distribution: Fake vs Real News") +
xlab("Target (0 = Fake, 1 = Real)") +
ylab("Count") +
theme_minimal()
# Line chart for trends (using numeric column averages over rows)
numeric_means <- colMeans(numeric_cols, na.rm = TRUE)
numeric_means_df <- data.frame(Feature = names(numeric_means), MeanValue = numeric_means)
ggplot(numeric_means_df, aes(x = Feature, y = MeanValue)) +
geom_line(aes(group = 1), color = "green") +  # Add group aesthetic to ensure a single line
geom_point(color = "red", size = 2) +
ggtitle("Mean Value of Numeric Features") +
xlab("Features") +
ylab("Mean Value") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Correlation Analysis
correlation1 <- cor(numeric_cols)
corrplot(correlation1, method = "circle")
# Feature Selection
selected_features1 <- c(names(numeric_cols), "target")
data1 <- data1[, selected_features1, drop = FALSE]
# Split Data
set.seed(123)
train_index1 <- createDataPartition(data1$target, p = 0.8, list = FALSE)
train_data1 <- data1[train_index1, ]
test_data1 <- data1[-train_index1, ]
# Random Forest Model
rf_model1 <- randomForest(target ~ ., data = train_data1, ntree = 100)
rf_pred1 <- predict(rf_model1, newdata = test_data1)
rf_pred1 <- factor(rf_pred1, levels = levels(test_data1$target))
# Support Vector Machine (SVM) Model
svm_model1 <- svm(target ~ ., data = train_data1, kernel = "linear", probability = TRUE)
svm_pred1 <- predict(svm_model1, newdata = test_data1)
# Model Evaluation
# Random Forest
rf_cm <- confusionMatrix(as.factor(rf_pred1), test_data1$target)
cat("Random Forest Confusion Matrix:\n")
print(rf_cm)
# SVM
svm_cm <- confusionMatrix(as.factor(svm_pred1), test_data1$target)
cat("SVM Confusion Matrix:\n")
print(svm_cm)
# Results Summary: Fake vs Real News
fake_news_count <- sum(test_data1$target == 0)
real_news_count <- sum(test_data1$target == 1)
cat(sprintf("Number of Fake News: %d\n", fake_news_count))
cat(sprintf("Number of Real News: %d\n", real_news_count))
